{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Advanced Analytics Showcase - Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font style=\"font-family:Trebuchet MS;\">\n",
    "\n",
    "# Leaning Objectives - By the end of this session, you will be able to...\n",
    "\n",
    "* Recognize common use cases of clustering in the industry\n",
    "* Understand the key concepts of clustering\n",
    "* Explain the basic clustering technique - KMeans\n",
    "* Appreciate the advantages of clustering over Excel-based segmentation technique, e.g., RFM\n",
    "\n",
    "<img src = \"images/clusteringcartoon3.png\" height = 400 width = 600><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font style=\"font-family:Trebuchet MS;\">\n",
    "\n",
    "# Introduction to Clustering\n",
    "\n",
    "<font size = \"4\"><b>Before we start with Clustering, let us understand first what is a cluster.</b></font>\n",
    "\n",
    "<b>Cluster</b> is the collection of data objects which are similar to one another within the same group (class or category) and are different from the objects in the other clusters.\n",
    "\n",
    "Clustering is an unsupervised learning technique in which there is predefined classes and prior information which defines how the data should be grouped or labeled into separate classes\n",
    "\n",
    "It could also be considered as Exploratory Data Analysis (EDA) process which help us to discover hidden patterns of interest or structure in data.\n",
    "\n",
    "Clustering can also work as a standalone tool to get the insights about the data distribution or as a preprocessing step in other algorithms.\n",
    "\n",
    "<i>RFM segmentation</i> is an example of clustering using transactional data. There are more advanced and powerful clustering techniques, which could help you derive better customer segmentations. We will demonstrate one of those in this session\n",
    "\n",
    "<font size = \"4\"><b>Let us talk about a simple example of clustering.</b></font><br>\n",
    "\n",
    "How would you distinguish between Computer Science (CS) experts, Maths experts and Machine Learning (ML) experts if you have the number of Maths and Statistics publications as well as the number of Computer Science publications published by them.\n",
    "In this instance clustering can be immensely useful as it will separate the three classes based on the number of Maths and CS publications they had.\n",
    "<br><br>\n",
    "<table><tr>\n",
    "<td><img src = \"images/cluster1.png\" height = \"300\" width = \"300\"/></td>\n",
    "<td><img src = \"images/cluster2.png\" height = \"300\" width = \"300\"/></td>\n",
    "</tr></table>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font style=\"font-family:Trebuchet MS;\">\n",
    "    \n",
    "## Possible Applications\n",
    "\n",
    "Clustering algorithms can be applied in many fields, for instance:\n",
    "\n",
    "1. __Marketing__: finding groups of customers with similar behavior given a large database of customer data containing their properties and past buying records\n",
    "2. __Biology__: classification of plants and animals given their features\n",
    "3. __Libraries__: book ordering\n",
    "4. __Insurance__: identifying groups of motor insurance policy holders with a high average claim cost; identifying frauds\n",
    "5. __City planning__: identifying groups of houses according to their house type, value and geographical location\n",
    "5. __Earthquake studies__: clustering observed earthquake epicenters to identify dangerous zones\n",
    "\n",
    "Examples of clustering used in client cases:\n",
    "1. __D2H (Direct-to-Home)__: Customer segmentation for a D2H provider based on the features of the packages provided and customer usage\n",
    "2. __Retail__: Clustering of retail stores having similar potential based on different demographic, sales features\n",
    "3. __Telecommunication:__ Clustering of the subscribers of a Telco firm based on usage specific variables to provide cross-sell and up-sell recommendations\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font style=\"font-family:Trebuchet MS;\">\n",
    "\n",
    "## How do we define good Clustering algorithms?\n",
    "\n",
    "High quality clusters can be created by reducing the distance between the objects in the same cluster known as intra-cluster minimization and increasing the distance with the objects in the other cluster known as inter-cluster maximization.\n",
    "\n",
    "<b>Intra-cluster Minimization:</b> The closer the objects in a cluster, the more likely they belong to the same cluster.\n",
    "\n",
    "<b>Inter-cluster Maximization:</b> This makes the separation between two clusters. The main goal is to maximize the distance between 2 clusters.\n",
    "\n",
    "<img src = \"images/good_clusters.jpeg\" height=\"700\" width=\"700\">\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font style=\"font-family:Trebuchet MS;\">\n",
    "\n",
    "## Distance Metrics\n",
    "\n",
    "Clustering is the grouping of similar instances/objects, some sort of measure that can determine whether two objects are similar or dissimilar is required. Many clustering methods use distance measures to determine the similarity or dissimilarity between any pair of objects.\n",
    "\n",
    "Some commonly used distance metrics are given below\n",
    "\n",
    "1. __Euclidean Distance:__ The Euclidean distance or Euclidean metric is the ordinary distance between two points that one would measure with a ruler. It is the straight line distance between two points. In a plane with p1 at (x1, y1) and p2 at (x2, y2), it is v((x1 - x2)² + (y1 - y2)²). This is the distance measure used as a default in most common clustering techniques like K-Means. The distance is calculated using the formula: $d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_{i}-y_{i})^2}$\n",
    "2. __Squared Euclidean Distance:__ The Squared Euclidean distance metric uses the same equation as the Euclidean distance metric, but does not take the squared root. $d(x, y) = \\sum_{i=1}^{n}(x_{i}-y_{i})^2$\n",
    "3. __Manhattan Distance:__ It is also known as City block distance, and absolute value distance or L1 distance. Manhattan  distances a distance that follows a route along the non-hypotenuse sides of a triangle. The name refers to the grid-like layout of most American cities which makes it impossible to go directly between two points. This metric is less affected by outliers than the Euclidean and squared Euclidean metrics. $d(x, y) = \\sum_{i=1}^{n}|x_{i}-y_{i}|$\n",
    "\n",
    "The choice of distance measures is very important, as it has a strong influence on the clustering results. For most common clustering software, the default distance measure is the Euclidean distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font style=\"font-family:Trebuchet MS;\">\n",
    "\n",
    "## Different types of clustering methods\n",
    "<img src = \"images/clusterclassification.png\" height = \"500\" width = \"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font style=\"font-family:Trebuchet MS;\">\n",
    "\n",
    "## Partitioning-Based Clustering Vs Hierarchical Clustering\n",
    "### Partitioning-Based\n",
    "\n",
    "<img src = \"images/partitioning.png\" height = 400 width = 400>\n",
    "\n",
    "* Partitioning-Based clustering algorithms generate various partitions and then evaluate them by some criterion. They are also referred to as non-hierarchical as each instance is placed in exactly one of k mutually exclusive clusters\n",
    "* Because only one set of clusters is the output of a typical partitioning-based clustering algorithm, the user is required to input the desired number of clusters (usually called k)\n",
    "* One of the most commonly used partitioning-based clustering algorithms is the k-means clustering algorithm. User is required to provide the number of clusters (k) before starting and the algorithm first initiates the centers (or centroids) of the k partitions\n",
    "\n",
    "### Hierarchical\n",
    "\n",
    "<img src = \"images/hierarchicalpic.png\" height = 400 width = 400>\n",
    "\n",
    "* Hierarchical clustering algorithms repeat the cycle of either merging smaller clusters in to larger ones or dividing larger clusters to smaller ones. Either way, it produces a hierarchy of clusters called a dendogram\n",
    "* Agglomerative clustering strategy uses the bottom-up approach of merging clusters in to larger ones, while divisive clustering strategy uses the top-down approach of splitting in to smaller ones\n",
    "* Dendrogram is a visual representation of the clusters, which displays the hierarchy very clearly. The user can obtain different clustering depending on the level at which the dendrogram is cut\n",
    "\n",
    "### What is the difference between Hierarchical and Partitional Clustering?\n",
    "\n",
    "|                | **Hierarchical**                                                     | **Partitional**                                      |\n",
    "|----------------|----------------------------------------------------------------------|------------------------------------------------------|\n",
    "| Runtime        | Slower                                                               | Faster                                               |\n",
    "| Initialisation | Distance metrics                                                     | Distance metrics Number of clusters Initial clusters |\n",
    "| Suitable data  | Categorical data                                                     | Numerical data                                       |\n",
    "| Output         | Dendogram hierarchy which requires subjective division into clusters | Exact K clusters                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font style=\"font-family:Trebuchet MS;\">\n",
    "\n",
    "# K-Means\n",
    "\n",
    "## What is K-Means?\n",
    "\n",
    "K-means clustering is a simple unsupervised learning algorithm that is used to solve clustering problems. It follows a simple procedure of classifying a given data set into a number of clusters, defined by the letter \"k,\" which is fixed beforehand. The clusters are then positioned as points and all observations or data points are associated with the nearest cluster, computed, adjusted and then the process starts over using the new adjustments until a desired result is reached.\n",
    "\n",
    "Following example talks about the basic steps in K-Means clustering which is generally used to cluster our data\n",
    "\n",
    "1. Start with number of clusters we want e.g., 3 in this case. K-Means algorithm start the process with random centers in data, and then tries to attach the nearest points to these centers\n",
    "2. Algorithm then moves the randomly allocated centers to the means of created groups\n",
    "3. In the next step, data points are again reassigned to these newly created centers\n",
    "4. Steps 2 & 3 are repeated until no member changes their association/groups or the time budget is running out\n",
    "\n",
    "[Interactive Demo of K-Means Algorithm](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)\n",
    "\n",
    "## When do we use K-Means?\n",
    "\n",
    "* K-Means is useful when we have an idea of how many clusters actually exists in your space\n",
    "* With a large number of variables, K-Means is computationally faster than other clustering techniques (if K is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# K-Means Practice\n",
    "\n",
    "**How to run a Jupyter Notebook?**\n",
    "\n",
    "*Key in values*\n",
    "You can click on the cell and insert text to the code cell. It is similar to how you write text in Word documents\n",
    "\n",
    "*Execution*\n",
    "Select the code cell and press \"Shift\" + \"Enter\" keys together on the keyboard. The codes in the cell will be executed\n",
    "Please note that some cells might take longer time to run, hence you might need to wait for a few seconds before seeing the outcomes\n",
    "\n",
    "Practice on the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PRACTICE_TEXT = \"REPLACE ME WITH YOUR NAME HERE\"\n",
    "print(\"Hi {}! Welcome to the clustering showcase session. You have learnt how to execute a Jupyter code cell!\".format(PRACTICE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing required Python libraries for this session\n",
    "# Press Shift + Enter\n",
    "\n",
    "!wget https://raw.githubusercontent.com/RISEBCG/DAB/main/M3A2-clustering/cluster_helper.py\n",
    "from cluster_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset prepared for clustering.\n",
    "# Press Shift + Enter\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/RISEBCG/DAB/main/M3A2-clustering/customer_clustering_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a glance at the data\n",
    "# .head() command shows the top 10 rows of the dataset\n",
    "# Key in df.head(10) and press Shift + Enter\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the top 10 rows, we can tell that the dataset contains the following columns in 4 categories:\n",
    "\n",
    "*ID:*\n",
    "- customer_id\n",
    "\n",
    "*Demographics:*\n",
    "- age\n",
    "- annual income\n",
    "\n",
    "*RFM features:*\n",
    "- most_recent_purchase_days\n",
    "- total_net_sales\n",
    "- total_no_transaction\n",
    "- overall seg\n",
    "\n",
    "*Purchase behaviours:*\n",
    "- total_sku_quantities\n",
    "- Greek yogurt_count\n",
    "- Nuts_count\n",
    "- Apple slices_count\n",
    "- Chips_count\n",
    "- Soda_count\n",
    "- Chocolate bar_count\n",
    "- Seasonal special_count\n",
    "- Promotion item_count\n",
    "- Lunchbox - Beef_count\n",
    "- Lunchbox - Vegan_count\n",
    "\n",
    "The next step is to examine if there are any outliers, missing values or any other issues with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We use .describe() to generate a data summary for all the numerical values.\n",
    "# From the data summary, we could easily spot any missing values or outliers\n",
    "# Key in df.describe() and press Shift + Enter\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Luckily, there are no missing values in this dataset, nor any extreme outliers. However, we did spot one issue - data scaling\n",
    "The numeric values in the dataframe have different scales, e.g., age ranges from 16 to 55. total_net_sales has a maximum of 56k.\n",
    "This will cause the issue of large scale variables dominating the distance metrics from a mathematical perspective.\n",
    "\n",
    "Assuming two data points with age 25 and 45, annual income 10k and 11k.\n",
    "One might argue that the annual income difference is less significant than the age difference\n",
    "Recall the Euclidean distance formula: $d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_{i}-y_{i})^2}$\n",
    "Using the formula, the distance will be computed as $\\sqrt{(45-25)^2 + (11000-10000)^2} = \\sqrt{1000400}$, with the annual income difference contributing to the majority of the distance\n",
    "\n",
    "This is counter-intuitive and will affect the quality of clustering. We can fix this through **data standardization**\n",
    "\n",
    "In this session, we will use Min-Max Standardization, which scales the numerical values from 0-1. More details can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data standardization can be applied to numerical values\n",
    "# Press Shift + Enter\n",
    "\n",
    "numeric_columns = ['age', 'annual_income', 'total_sku_quantities',\n",
    "                   'total_net_sales', 'most_recent_purchase_days', 'total_no_transactions',\n",
    "                   'Greek yogurt_count', 'Nuts_count', 'Apple slices_count', 'Chips_count',\n",
    "                   'Soda_count', 'Chocolate bar_count', 'Seasonal special_count',\n",
    "                   'Promotion item_count', 'Lunchbox - Beef_count', 'Lunchbox - Vegan_count']\n",
    "\n",
    "df, df_original = standardize_numeric_data(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now, the data is ready for clustering!\n",
    "# From RFM analysis, the customers were segmented in to 4 groups\n",
    "# Let's start with the same number of segment\n",
    "# Recall RFM only takes in the 3 columns recency, frequency and monetary values\n",
    "# In K-means, we can use more variables for clustering - in this case, let's use all the numeric values\n",
    "# Press Shift + Enter\n",
    "\n",
    "df_pred = run_kmeans_model(4, df, df_original, numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the output dataframe\n",
    "# The last column indicate the cluster that the customer is assigned to based on K-Means clustering\n",
    "# Key in df_pred.head(10) and press Shift + Enter\n",
    "# Note: In Python, number usually starts with 0. Hence, the 4 clusters are cluster 0, 1, 2 and 3\n",
    "\n",
    "df_pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# How would you visualise the clusters?\n",
    "# This dataset is multi-dimension, i.e., contains many columns\n",
    "# To visualise each data point in a 3-D space, you will need to use the technique \"Dimensionality Reduction\"\n",
    "# This technique summarises the existing dimensions into 3-D dataframe, within minimal loss of information\n",
    "# Press Shift + Enter - it might take slightly longer time to run\n",
    "\n",
    "create_cluster_visualisation(df_pred, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each dot represents a data point and each color represents the cluster assignment. You can drag the visualisation to view these data points from different angles. You can also zoom in and out.\n",
    "\n",
    "You might have realised that while some clusters are more distinctive, data points from different cluster could still mix together in the 3-D space. In the most ideal scenario, you will want to see mutually exclusive clusters. So, why does this happen?\n",
    "\n",
    "There are several possible explanations:\n",
    "- Precision is lost during the dimensionality reduction\n",
    "- KMeans model can be fine-tuned further to achieve better distinction\n",
    "- There's no natural groups in the datasets\n",
    "\n",
    "Now. Let's deep dive into each of these clusters and observe what are they."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the first cluster from the KMeans model\n",
    "# Press Shift + Enter\n",
    "\n",
    "create_segment_profiling_chart(df_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You might have realised - this is very similar to the Star segment we have identified from the RFM analysis!\n",
    "- These are mostly working adults, with relatively high income\n",
    "- Favorite SKUs are healthy food - Nuts, Greek Yogurt and Apple Slices\n",
    "- High in Monetary, Frequency and Recency values\n",
    "\n",
    "However, note that the segment size is significantly lower than our Star segment - it only has 18 customers. Can you think of potential reasons?\n",
    "\n",
    "Can you create the same chart for the other 3 clusters created by the KMeans model?\n",
    "Replace the 0 in the following cells with other cluster numbers, i.e., 1, 2, and 3. Press Shift+Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For cluster 1 profile\n",
    "\n",
    "# create_segment_profiling_chart(df_pred, 0)\n",
    "create_segment_profiling_chart(df_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For cluster 2 profile\n",
    "\n",
    "# create_segment_profiling_chart(df_pred, 0)\n",
    "create_segment_profiling_chart(df_pred, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For cluster 3 profile\n",
    "\n",
    "# create_segment_profiling_chart(df_pred, 0)\n",
    "create_segment_profiling_chart(df_pred, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Discussion**\n",
    "- How are the clusters from KMeans different from RFM?\n",
    "- What are the new insights you can get from these KMeans clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "**How do you evaluate the clusters?**\n",
    "\n",
    "How can you be sure that 4 is the right number of clusters? You can evaluate the clusters by using SSE and Elbow Method\n",
    "\n",
    "SSE is the Sum of the Squared Error differences between each observation and its group's mean. It can be used as a measure of variation within a cluster. If all cases within a cluster are identical the SSE would then be equal to 0.\n",
    "\n",
    "$SSE = \\sum_{i=1}^{K} \\sum_{x_j \\in C_i} | x_j - \\mu_i |^2$\n",
    "\n",
    "In this formula, $K$ is the number of clusters, $C_i$ is the $i$th cluster, $\\mu_i$ is the centroid (mean) of the $i$th cluster, $x_j$ is the $j$th data point in the $i$th cluster, and $|\\cdot|$ denotes the Euclidean distance between a data point and its cluster centroid.\n",
    "\n",
    "<b>Algorithm for Elbow Method</b>\n",
    "\n",
    "1. Run k-means clustering on the dataset for a range of values of k (say, k from 1 to 10 in the examples above)\n",
    "2. For each value of k calculate the sum of squared errors (SSE)\n",
    "3. Plot a line chart of the SSE for each value of k\n",
    "4. If the line chart looks like an arm, then the \"elbow\" on the arm is the value of k that is the best.\n",
    "\n",
    "The plot should look like this:\n",
    "<img src = \"images/elbow1.png\"><br>\n",
    "\n",
    "The idea is that we want a small SSE, but that the SSE tends to decrease toward 0 as we increase k (the SSE is 0 when k is equal to the number of data points in the dataset, because then each data point is its own cluster, and there is no error between it and the center of its cluster).\n",
    "So our goal is to choose a small value of k that still has a low SSE, and the elbow usually represents where we start to have diminishing returns by increasing k.\n",
    "\n",
    "In our plot we see a pretty clear elbow at k = 3, indicating that 3 is the best number of clusters.\n",
    "\n",
    "Let's apply this algorithm on our models now!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create the elbow chart for this problem!\n",
    "# Press Shift + Enter\n",
    "\n",
    "plot_elbow_chart(df_pred, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the plot, 2 is the \"clear elbow\" of the chart. Shall we re-run the algorithm with number of clusters = 2?\n",
    "\n",
    "Well, you have to consider the following questions before making this call:\n",
    "- Is it enough to have just 2 clusters? Will you ignore some smaller segments among your customers?\n",
    "- Are the 2 clusters identified suitable for marketing purpose?\n",
    "\n",
    "In reality, the elbow algorithm serves more as a reference. You might want to iterate the KMeans with different parameters to create the most meaningful clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Explore different KMeans setup**\n",
    "\n",
    "Different choices of numbers of clusters will lead to different cluster assignments, which may bring out different insights about customer segmentations.\n",
    "Let's use this exercise to explore different configurations to the KMeans model. Observe the different customer segments.\n",
    "\n",
    "In the next cell, replace number_of_clusters with your choice.\n",
    "- number_of_clusters: an integer, recommended range is from 2 to 6\n",
    "\n",
    "Hint:\n",
    "- Try number_of_clusters 5 and observe the outcome clusters\n",
    "- What are the issues with some clusters (check the segment size)\n",
    "- What can you do with these clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Replace number_of_clusters with your choice\n",
    "# Explore the output clusters, observe differences, and idenfity new insights\n",
    "# Press Shift + Enter\n",
    "\n",
    "# experiment_different_cluster(df, df_original, cluster_columns, number_of_clusters)\n",
    "experiment_different_cluster(df, df_original, numeric_columns, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Comparison between RFM and K-Means\n",
    "\n",
    "Hopefully, by now you have a good understanding of K-Means in terms of its concept, algorithm, and implementation.\n",
    "Let's compare K-Means with the RFM analysis that you are familiar with:\n",
    "<br>\n",
    "\n",
    "|                         | K-Means                                          | RFM                                         |\n",
    "|-------------------------|--------------------------------------------------|---------------------------------------------|\n",
    "| Segment definition      | The algorithm discovers natural groups           | User has to define each segment             |\n",
    "| Column inputs           | Any numeric variables                            | Only RFM columns                            |\n",
    "| Limit to No. segments   | No theoretical limit                             | Difficult to define more than 8 segments    |\n",
    "| Prune to bias           | Model decides segments based on distance metrics | Manual judgment and assumptions             |\n",
    "| Time to iterate         | Fast                                             | Slow                                        |\n",
    "| Platform                | Programming languages, off-the-shelf products    | Excel                                       |\n",
    "| Scale                   | Suitable for large datasets of all sizes         | Suitable for small to medium datasets       |\n",
    "| Sensitivity to outliers | Sensitive                                        | Not sensitive                               |\n",
    "| Interpretability        | Limited - clusters may not have intuitive labels | High - segments are based on business logic |\n",
    "<br>\n",
    "\n",
    "As you can tell, K-Means overpass RFM in most of the dimensions, with its own limitations:\n",
    "- K-Means is sensitive to outliers - outliers will still be included in the clusters and will affect new cluster center:\n",
    "    -  Solution: Use distance threshold or other outlier detection algorithms to detect noise data and outliers\n",
    "- K-Means clusters are not self-explanatory\n",
    "    -  Solution: Deep dive into each cluster to identify their characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font style=\"font-family:Trebuchet MS;\">\n",
    "\n",
    "***\n",
    "\n",
    "*This marks the end of this lesson*<br><br>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"8\"><font style=\"font-family:Trebuchet MS;\">Happy Clustering !!!</font></font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "544px",
    "left": "133px",
    "top": "203.14px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}